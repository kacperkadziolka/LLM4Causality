{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-10T17:42:26.606084Z",
     "start_time": "2024-12-10T17:42:26.603749Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm \n",
    "    \n",
    "from datasets import load_dataset"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:42:29.730837Z",
     "start_time": "2024-12-10T17:42:26.652650Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Download the CORR2CAUSE dataset\n",
    "dataset_name = \"causal-nlp/corr2cause\"\n",
    "try:\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    print(\"Dataset successfully loaded.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading the dataset: {e}\")"
   ],
   "id": "85ec003a25bf94",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset successfully loaded.\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:42:29.999204Z",
     "start_time": "2024-12-10T17:42:29.747508Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Access the train, test, and validation splits\n",
    "train_dataset = dataset['train']\n",
    "test_dataset = dataset['test']\n",
    "validation_dataset = dataset['validation']\n",
    "\n",
    "# Convert to Pandas DataFrames\n",
    "train_df = train_dataset.to_pandas()\n",
    "test_df = test_dataset.to_pandas()\n",
    "validation_df = validation_dataset.to_pandas()\n",
    "\n",
    "# Display the length of each split\n",
    "print(f\"Train split length: {len(train_df)}\")\n",
    "print(f\"Test split length: {len(test_df)}\")\n",
    "print(f\"Validation split length: {len(validation_df)}\")"
   ],
   "id": "bf3685ff0cdf4773",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split length: 205734\n",
      "Test split length: 1162\n",
      "Validation split length: 1076\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:42:30.074117Z",
     "start_time": "2024-12-10T17:42:30.031744Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Function to extract only problem with given amount of num_variables [4-6]\n",
    "def filter_by_num_variables(df, target_num):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame to include only rows where the 'num_variables' column\n",
    "    equals the specified target number.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): The input DataFrame containing a 'num_variables' column.\n",
    "    - target_num (int): The number of variables to filter by.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: A filtered DataFrame with rows where 'num_variables' == target_num.\n",
    "    \"\"\"\n",
    "    if 'num_variables' not in df.columns:\n",
    "        raise ValueError(\"The DataFrame does not contain a 'num_variables' column.\")\n",
    "    \n",
    "    # Ensure that 'num_variables' column contains numeric data\n",
    "    if not pd.api.types.is_numeric_dtype(df['num_variables']):\n",
    "        raise TypeError(\"'num_variables' column must be of a numeric type.\")\n",
    "    \n",
    "    # Filter the DataFrame\n",
    "    filtered_df = df[df['num_variables'] == target_num].copy()\n",
    "    \n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "train_df = filter_by_num_variables(train_df, 4)\n",
    "print(f\"Train split length: {len(train_df)}\")"
   ],
   "id": "36c5006a32276840",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train split length: 576\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:55:45.161727Z",
     "start_time": "2024-12-10T17:55:45.151950Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def parse_input(text):\n",
    "    # Initialize containers\n",
    "    variables = set()\n",
    "    correlations = []\n",
    "    marginal_independencies = []\n",
    "    conditional_independencies = []\n",
    "\n",
    "    # Extract variables\n",
    "    var_match = re.search(r'variables?(.*?)[\\.\\n]', text)\n",
    "    if var_match:\n",
    "        vars_text = var_match.group(1)\n",
    "        # Split vars_text by commas and 'and'\n",
    "        vars_list = re.split(r',\\s*|\\s+and\\s+|\\s*,\\s*', vars_text)\n",
    "        vars_list = [var.strip() for var in vars_list if var.strip() and var.strip().lower() != 'and']\n",
    "        variables.update(vars_list)\n",
    "    else:\n",
    "        # Handle case when variables not found\n",
    "        pass\n",
    "\n",
    "    # Extract correlations\n",
    "    # The correlations are in the text after 'All the statistical relations among these variables are as follows:'\n",
    "    # and before 'However,'\n",
    "    correlations_text_match = re.search(r'All the statistical relations.*?are as follows:(.*?)(However|$)', text, re.DOTALL)\n",
    "    if correlations_text_match:\n",
    "        correlations_text = correlations_text_match.group(1)\n",
    "        # Now we need to extract all 'X correlates with Y.'\n",
    "        correlation_matches = re.findall(r'([A-Za-z]+) correlates with ([A-Za-z]+)\\.', correlations_text)\n",
    "        correlations.extend(correlation_matches)\n",
    "        variables.update([var for pair in correlation_matches for var in pair])\n",
    "\n",
    "    # Extract independencies\n",
    "    # The independencies are in the text after 'However,'\n",
    "    independencies_text_match = re.search(r'However,(.*)', text, re.DOTALL)\n",
    "    if independencies_text_match:\n",
    "        independencies_text = independencies_text_match.group(1)\n",
    "        # Now split independencies into sentences\n",
    "        sentences = re.findall(r'([^.]*?\\.)', independencies_text)\n",
    "        for sentence in sentences:\n",
    "            sentence = sentence.strip()\n",
    "            # Try to match marginal independencies\n",
    "            marg_match = re.match(r'([A-Za-z]+) is independent of ([A-Za-z]+)\\.', sentence)\n",
    "            if marg_match:\n",
    "                var1 = marg_match.group(1)\n",
    "                var2 = marg_match.group(2)\n",
    "                marginal_independencies.append((var1, var2))\n",
    "                variables.update([var1, var2])\n",
    "            else:\n",
    "                # Try to match conditional independencies\n",
    "                cond_match = re.match(r'([A-Za-z]+) and ([A-Za-z]+) are independent given (.*?)[\\.\\n]', sentence)\n",
    "                if cond_match:\n",
    "                    var1 = cond_match.group(1)\n",
    "                    var2 = cond_match.group(2)\n",
    "                    given_vars_text = cond_match.group(3)\n",
    "                    # Split given_vars_text by commas and 'and', strip spaces\n",
    "                    given_vars = re.split(r',\\s*|\\s+and\\s+', given_vars_text)\n",
    "                    given_vars = [var.strip() for var in given_vars if var.strip() and var.strip().lower() != 'and']\n",
    "                    conditional_independencies.append({\n",
    "                        'vars': (var1, var2),\n",
    "                        'given': given_vars\n",
    "                    })\n",
    "                    variables.update([var1, var2] + given_vars)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "    return {\n",
    "        'variables': list(sorted(variables)),\n",
    "        'correlations': correlations,\n",
    "        'marginal_independencies': marginal_independencies,\n",
    "        'conditional_independencies': conditional_independencies\n",
    "    }"
   ],
   "id": "33620ebe6b2fd5ef",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T18:00:07.207646Z",
     "start_time": "2024-12-10T18:00:07.194459Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def construct_causal_skeleton_with_steps(parsed_data):\n",
    "    variables = parsed_data['variables']\n",
    "    correlations = parsed_data['correlations']\n",
    "    marginal_independencies = parsed_data['marginal_independencies']\n",
    "    conditional_independencies = parsed_data['conditional_independencies']\n",
    "\n",
    "    reasoning_steps = []\n",
    "\n",
    "    # Step 1: Read the Data\n",
    "    reasoning_steps.append(\"Step 1: Read the Data\\n\")\n",
    "    reasoning_steps.append(f\"- Extracted variables: {', '.join(variables)}\")\n",
    "    correlations_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in correlations])\n",
    "    reasoning_steps.append(f\"- Correlations: {correlations_str}\")\n",
    "    marg_indep_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in marginal_independencies])\n",
    "    reasoning_steps.append(f\"- Marginal Independencies: {marg_indep_str}\")\n",
    "    cond_indep_str = '\\n  - '.join([\n",
    "        f\"({indep['vars'][0]}, {indep['vars'][1]}) are independent given {', '.join(indep['given'])}\"\n",
    "        for indep in conditional_independencies\n",
    "    ])\n",
    "    reasoning_steps.append(f\"- Conditional Independencies:\\n  - {cond_indep_str}\\n\")\n",
    "\n",
    "    # Step 2: Initialize the Graph\n",
    "    reasoning_steps.append(\"Step 2: Initialize the Graph\\n\")\n",
    "    edges = set()\n",
    "    for var1, var2 in correlations:\n",
    "        edges.add(frozenset([var1, var2]))\n",
    "    initial_edges_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in correlations])\n",
    "    reasoning_steps.append(f\"- Created edges between all correlated variable pairs.\")\n",
    "    reasoning_steps.append(f\"- Initial edges: {{{initial_edges_str}}}\\n\")\n",
    "\n",
    "    # Step 3: Apply Marginal Independencies\n",
    "    reasoning_steps.append(\"Step 3: Apply Marginal Independencies\\n\")\n",
    "    removed_edges = set()\n",
    "    for var1, var2 in marginal_independencies:\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges.add(edge)\n",
    "            reasoning_steps.append(f\"- **Because {var1} is independent of {var2}, there is no edge between {var1} and {var2}.**\")\n",
    "    if not removed_edges:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\\n\")\n",
    "    else:\n",
    "        reasoning_steps.append(\"\")\n",
    "\n",
    "    # Step 4: Apply Conditional Independencies\n",
    "    reasoning_steps.append(\"Step 4: Apply Conditional Independencies\\n\")\n",
    "    removed_edges_cond = set()\n",
    "    for indep in conditional_independencies:\n",
    "        var1, var2 = indep['vars']\n",
    "        edge = frozenset([var1, var2])\n",
    "        if edge in edges:\n",
    "            edges.remove(edge)\n",
    "            removed_edges_cond.add(edge)\n",
    "            given_vars_str = ', '.join(indep['given'])\n",
    "            reasoning_steps.append(f\"- Because {var1} and {var2} are independent given {given_vars_str}, there is no edge between {var1} and {var2}.\")\n",
    "    if not removed_edges_cond:\n",
    "        reasoning_steps.append(\"- No edges removed in this step.\\n\")\n",
    "    else:\n",
    "        reasoning_steps.append(\"\")\n",
    "        \n",
    "    # Step 5: Compile the Causal Undirected Skeleton\n",
    "    reasoning_steps.append(\"Step 5: Compile the Causal Undirected Skeleton\")\n",
    "    final_edges = {}\n",
    "    for edge in edges:\n",
    "        var1, var2 = sorted(edge)\n",
    "        if var1 not in final_edges:\n",
    "            final_edges[var1] = []\n",
    "        final_edges[var1].append(var2)\n",
    "\n",
    "    reasoning_steps.append(\"In this graph:\")\n",
    "    for node in variables:\n",
    "        if node in final_edges:\n",
    "            reasoning_steps.append(f\"  - Node {node} is connected to nodes {', '.join(final_edges[node])}.\")\n",
    "        else:\n",
    "            reasoning_steps.append(f\"  - Node {node} has no connections.\")\n",
    "\n",
    "    # Step 5: Compile the Remaining Edges\n",
    "    # reasoning_steps.append(\"Step 5: Compile the Remaining Edges\\n\")\n",
    "    # remaining_edges = [(list(edge)[0], list(edge)[1]) for edge in edges]\n",
    "    # remaining_edges_str = ', '.join([f\"({var1}, {var2})\" for var1, var2 in remaining_edges])\n",
    "    # reasoning_steps.append(f\"- Remaining edges after applying independencies: {{{remaining_edges_str}}}\\n\")\n",
    "\n",
    "    # Step 6: Prepare the Step-by-Step Answer\n",
    "    # reasoning_steps.append(\"Step 6: Give the Final Answer with Computed Causal Undirected Skeleton\\n\")\n",
    "    # reasoning_steps.append(f\"  - Edges: {{{remaining_edges_str}}}\")\n",
    "\n",
    "    answer = '\\n'.join(reasoning_steps)\n",
    "    return answer"
   ],
   "id": "f0fa3399b55867c5",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:55:53.995562Z",
     "start_time": "2024-12-10T17:55:53.991348Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def generate_causal_skeleton_reasoning(text):\n",
    "    parsed_data = parse_input(text)\n",
    "    answer = construct_causal_skeleton_with_steps(parsed_data)\n",
    "    return answer"
   ],
   "id": "d710c104d5512f6e",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T18:00:12.030487Z",
     "start_time": "2024-12-10T18:00:12.027322Z"
    }
   },
   "cell_type": "code",
   "source": [
    "result = generate_causal_skeleton_reasoning(train_df.iloc[0]['input'])\n",
    "print(result)"
   ],
   "id": "22d9d74090820f35",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Read the Data\n",
      "\n",
      "- Extracted variables: A, B, C, D\n",
      "- Correlations: (A, B), (A, C), (A, D), (B, C), (B, D), (C, D)\n",
      "- Marginal Independencies: \n",
      "- Conditional Independencies:\n",
      "  - (B, D) are independent given A\n",
      "  - (B, D) are independent given A, C\n",
      "  - (C, D) are independent given A\n",
      "  - (C, D) are independent given A, B\n",
      "\n",
      "Step 2: Initialize the Graph\n",
      "\n",
      "- Created edges between all correlated variable pairs.\n",
      "- Initial edges: {(A, B), (A, C), (A, D), (B, C), (B, D), (C, D)}\n",
      "\n",
      "Step 3: Apply Marginal Independencies\n",
      "\n",
      "- No edges removed in this step.\n",
      "\n",
      "Step 4: Apply Conditional Independencies\n",
      "\n",
      "- Because B and D are independent given A, there is no edge between B and D.\n",
      "- Because C and D are independent given A, there is no edge between C and D.\n",
      "\n",
      "Step 5: Compile the Causal Undirected Skeleton\n",
      "In this graph:\n",
      "  - Node A is connected to nodes B, D, C.\n",
      "  - Node B is connected to nodes C.\n",
      "  - Node C has no connections.\n",
      "  - Node D has no connections.\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T17:42:30.156060Z",
     "start_time": "2024-12-10T17:42:30.149317Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_dataframe_with_progress(df, input_column, output_column, processing_function, output_file):\n",
    "    \"\"\"\n",
    "    Processes a DataFrame by applying a function to a specified input column and saving the result in a new column.\n",
    "    Includes a progress bar to track the processing.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The DataFrame to process.\n",
    "        input_column (str): The name of the input column where the function will be applied.\n",
    "        output_column (str): The name of the new column to store the results.\n",
    "        processing_function (callable): The function to apply to each row's input column.\n",
    "        output_file (str): The file path to save the processed DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The modified DataFrame with the new column added.\n",
    "    \"\"\"\n",
    "    # Make a copy of the original DataFrame\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Enable the tqdm progress bar for pandas\n",
    "    tqdm.pandas(desc=f\"Processing {output_column}\")\n",
    "\n",
    "    # Apply the processing function with progress tracking\n",
    "    df_copy[output_column] = df_copy[input_column].progress_apply(processing_function)\n",
    "\n",
    "    # Save the modified DataFrame to a file, e.g., as a CSV\n",
    "    df_copy.to_csv(output_file, index=False)\n",
    "\n",
    "    print(f\"Processing complete and DataFrame saved to {output_file}.\")\n",
    "    return df_copy"
   ],
   "id": "bb04c2cc91bce94e",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-10T18:11:15.327752Z",
     "start_time": "2024-12-10T18:11:15.243491Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_train_df = process_dataframe_with_progress(\n",
    "    df=train_df,\n",
    "    input_column='input',\n",
    "    output_column='expected_answer',\n",
    "    processing_function=generate_causal_skeleton_reasoning,\n",
    "    output_file='train.csv'\n",
    ")"
   ],
   "id": "426b9a3cb3637424",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing expected_answer:   0%|          | 0/576 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "17474c955d0c4b41b4174b464b67a373"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and DataFrame saved to train.csv.\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-26T21:00:00.330011Z",
     "start_time": "2024-10-26T20:59:59.936272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processed_validation_df = process_dataframe_with_progress(\n",
    "    df=validation_df,\n",
    "    input_column='input',\n",
    "    output_column='expected_answer',\n",
    "    processing_function=generate_causal_skeleton_reasoning,\n",
    "    output_file='validation.csv'\n",
    ")"
   ],
   "id": "d46fa0399ff9d717",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Processing expected_answer:   0%|          | 0/1076 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "05b31aca545849f1b2c43e0f983f8773"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete and DataFrame saved to processed_validation_df.csv.\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
